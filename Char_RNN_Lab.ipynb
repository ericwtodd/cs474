{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cksgAH12XRjV"
   },
   "source": [
    "# Lab 6: Sequence-to-sequence models\n",
    "\n",
    "## Description:\n",
    "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
    "\n",
    "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
    "\n",
    "## There are two parts of this lab:\n",
    "###  1.   Wiring up a basic sequence-to-sequence computation graph\n",
    "###  2.   Implementing your own GRU cell.\n",
    "\n",
    "\n",
    "An example of my final samples are shown below (more detail in the\n",
    "final section of this writeup), after 150 passes through the data.\n",
    "Please generate about 15 samples for each dataset.\n",
    "\n",
    "<code>\n",
    "And ifte thin forgision forward thene over up to a fear not your\n",
    "And freitions, which is great God. Behold these are the loss sub\n",
    "And ache with the Lord hath bloes, which was done to the holy Gr\n",
    "And appeicis arm vinimonahites strong in name, to doth piseling \n",
    "And miniquithers these words, he commanded order not; neither sa\n",
    "And min for many would happine even to the earth, to said unto m\n",
    "And mie first be traditions? Behold, you, because it was a sound\n",
    "And from tike ended the Lamanites had administered, and I say bi\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2i_QpSsWG4c"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 0: Readings, data loading, and high level training\n",
    "\n",
    "---\n",
    "\n",
    "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
    "\n",
    "* Read the following\n",
    "\n",
    "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "l7bdZWxvJrsx",
    "outputId": "a7b3161e-8660-439f-f8ec-1f244834a76c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-18 02:20:53--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
      "Resolving piazza.com (piazza.com)... 52.2.48.133, 34.205.95.128, 3.214.17.10, ...\n",
      "Connecting to piazza.com (piazza.com)|52.2.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
      "--2019-10-18 02:20:53--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
      "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 13.224.12.211, 13.224.12.155, 13.224.12.150, ...\n",
      "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|13.224.12.211|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1533290 (1.5M) [application/x-gzip]\n",
      "Saving to: ‘./text_files.tar.gz’\n",
      "\n",
      "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2019-10-18 02:20:53 (33.0 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
      "\n",
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 4.9MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
      "file_len = 2579888\n"
     ]
    }
   ],
   "source": [
    "# Get the text files that will be used for training and testing and unzip them\n",
    "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
    "! tar -xzf text_files.tar.gz\n",
    "! pip install unidecode\n",
    "! pip install torch\n",
    "\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    " \n",
    "import pdb\n",
    " \n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "TxBeKeNjJ0NQ",
    "outputId": "b0f0a32c-5b9d-43cb-dea9-2b661ee0aa4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, sparkling \n",
      "with joy. And then he cast the leaves into the bowls of steaming water that \n",
      "were brought to him, and at once all hearts were lightened. For the \n",
      "fragrance that came to each was like a me\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "# Chunks are like one training example (similar to one image for a CNN)\n",
    "def random_chunk():\n",
    "  start_index = random.randint(0, file_len - chunk_len)\n",
    "  end_index = start_index + chunk_len + 1\n",
    "  return file[start_index:end_index]\n",
    "  \n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "On0_WitWJ99e",
    "outputId": "f6d9d47f-ecd3-43c5-afa8-8b19648768be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 13, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "  tensor = torch.zeros(len(string)).long()\n",
    "  for c in range(len(string)):\n",
    "      tensor[c] = all_characters.index(string[c])\n",
    "  return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcdDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYJPTLcaYmfI"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating your own GRU cell \n",
    "\n",
    "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
    "\n",
    "---\n",
    "\n",
    "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
    "\n",
    "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "* Create a custom GRU cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavAv50ZKQ-F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GRU(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(GRU, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "  \n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "    # Our 3 nn.Linear Modules represent the parameters our 6 Weight Matrices, as well as the\n",
    "    # biases that are present in the forward equation description. This is because nn.Linear has a default bias term that it keeps track of as well as its own\n",
    "    # matrix weights\n",
    "    self.W_r = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    self.W_z = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    self.W_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    \n",
    "  \n",
    "  def forward(self, input_x, hidden):\n",
    "    # Each layer does the following:\n",
    "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
    "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
    "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
    "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
    "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
    "    \n",
    "    r_t = self.sigmoid(self.W_r(torch.cat((input_x, hidden), dim=2)))\n",
    "    z_t = self.sigmoid(self.W_z(torch.cat((input_x, hidden), dim=2)))\n",
    "    h_tilde = self.tanh(self.W_h(torch.cat((input_x, torch.mul(r_t, hidden)), dim=2)))\n",
    "    h_t = torch.mul(z_t, hidden) + torch.mul((1-z_t), h_tilde)\n",
    "\n",
    "    return h_t, h_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtXdX-B_WiAY"
   },
   "source": [
    "---\n",
    "\n",
    "##  Part 1: Building a sequence to sequence model\n",
    "\n",
    "---\n",
    "\n",
    "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
    "\n",
    "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "* Create an RNN class that extends from nn.Module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6tNdEnzWj5F"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "    super(RNN, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    \n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = GRU(hidden_size, hidden_size, num_layers = n_layers) # Custom-built GRU only has 1 layer even though it takes the parameter num_layers.\n",
    "    # self.gru = nn.GRU(hidden_size, hidden_size, num_layers = n_layers)\n",
    "    self.decoder = nn.Linear(hidden_size, output_size) # Takes GRU output and turns it into a character output\n",
    "\n",
    "  def forward(self, input_char, hidden_state):\n",
    "    \"\"\"Uses the GRU architecture, combined with previous hidden states as the forward pass\"\"\"\n",
    "    encoded_input = self.embedding(input_char).view(1,1,-1)\n",
    "    output, hidden = self.gru(encoded_input, hidden_state)\n",
    "    out_decoded = self.decoder(output)\n",
    "    \n",
    "    return out_decoded, hidden\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrhXghEPKD-5"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "  chunk = random_chunk()\n",
    "  inp = char_tensor(chunk[:-1])\n",
    "  target = char_tensor(chunk[1:])\n",
    "  return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpiGObbBX0Mr"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Sample text and Training information\n",
    "\n",
    "---\n",
    "\n",
    "We now want to be able to train our network, and sample text after training.\n",
    "\n",
    "This function outlines how training a sequence style network goes. \n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "* Fill in the pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ALC3Pf8Kbsi"
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "  ## initialize hidden layers, set up gradient and loss \n",
    "    # your code here\n",
    "  ## /\n",
    "  decoder_optimizer.zero_grad()\n",
    "  hidden_state = decoder.init_hidden()\n",
    "  loss = 0\n",
    "  \n",
    "  # Go through all the characters in the chunk of text and predict on them and compute the loss\n",
    "  for i, input_char in enumerate(inp):\n",
    "    output, hidden_state = decoder(input_char, hidden_state)\n",
    "    loss += criterion(output.squeeze(1),target[i].unsqueeze(0))\n",
    "  \n",
    "  # Backpropogate and update the weights through the optimizer\n",
    "  loss.backward()\n",
    "  decoder_optimizer.step()\n",
    "  \n",
    "  return loss.item() / len(inp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EN06NUu3YRlz"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sample text and Training information\n",
    "\n",
    "---\n",
    "\n",
    "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
    "\n",
    "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "* Fill out the evaluate function to generate text frome a primed string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-bp-OZ1KjNh"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "  #Initialize hidden state and prediction string that will be returned\n",
    "  hidden_state = decoder.init_hidden()\n",
    "  prediction = prime_str\n",
    "  \n",
    "  # Use all of the input string except the last character to update our hidden state\n",
    "  for i in range(len(prime_str) - 1):\n",
    "    hidden_state = decoder(char_tensor(prime_str[i]), hidden_state)[1]  \n",
    "\n",
    "  for i in range(predict_len):\n",
    "    # Our first input character is the previous character of the current prediction\n",
    "    input_char = char_tensor(prediction[-1])\n",
    "    out, hidden_state = decoder(input_char, hidden_state)\n",
    "    # Turn out vector into output probabilities and sample from it using the multinomial distribution\n",
    "    scaled_output = torch.exp(out/temperature)\n",
    "    chosen_char_idx = torch.multinomial(scaled_output.squeeze(0).squeeze(0), 1, replacement=True)\n",
    "    # Get the predicted character and add it to the prediction string\n",
    "    next_character = all_characters[chosen_char_idx]\n",
    "    prediction += next_character\n",
    "  \n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Du4AGA8PcFEW"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: (Create a GRU cell, requirements above)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFS2bpHSZEU6"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 5: Run it and generate some text!\n",
    "\n",
    "---\n",
    "\n",
    "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "**DONE:**\n",
    "* Create some cool output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nXFeCmdKodw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "n_epochs = 2000\n",
    "print_every = 200\n",
    "plot_every = 10\n",
    "hidden_size = 200\n",
    "n_layers = 1\n",
    "lr = 0.001\n",
    " \n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xKfozqw-6eqb",
    "outputId": "a3558482-acf4-42a6-f3dd-ae2c7dd1074d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.48689937591553 (200 10%) 2.0389]\n",
      "Wh)ry' \n",
      "hat on the and whall laot ent thit hain \n",
      "\n",
      "Dand and the thron semun ong came dous waid oa and c \n",
      "\n",
      "[109.43169116973877 (400 20%) 2.0838]\n",
      "Whow. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "The lighted that we lime am his to the he or a \n",
      "you his shad \n",
      "sond core we dind. He may \n",
      "\n",
      "[162.79693484306335 (600 30%) 1.7647]\n",
      "Whcurso melt \n",
      "the storn went some were your could \n",
      "stoud they reach the Menmange we hid sein \n",
      "like wel \n",
      "\n",
      "[216.09594416618347 (800 40%) 1.7699]\n",
      "Whrar, his worch the Riding theK up worly dearl, and the Shever. 'He ourreare mes \n",
      "of the stray bento  \n",
      "\n",
      "[268.1515510082245 (1000 50%) 1.7462]\n",
      "Whe seed the pory; but where fries mysome mones. They were all bot be wonewards of Blood. 'I his his d \n",
      "\n",
      "[320.13880801200867 (1200 60%) 1.7671]\n",
      "Why! ' \n",
      "\n",
      "'You tam the encough tien it you of the Tarong things of the tood and \n",
      "round of the was a lik \n",
      "\n",
      "[372.27011156082153 (1400 70%) 1.6499]\n",
      "When, That the road ingeth. All dismen gotto in the comirs \n",
      "lould, lbevone, and fell enside the with h \n",
      "\n",
      "[424.2706649303436 (1600 80%) 1.6901]\n",
      "Whmer mest in shorse to need awained hisker became fingash, shofe. No the his leck. Did looks \n",
      "as So m \n",
      "\n",
      "[476.2551531791687 (1800 90%) 1.7939]\n",
      "Wht company. 'I was shadow as I came dood. I were that ' seeming \n",
      "of the End at its. It we comore eyes \n",
      "\n",
      "[528.5238394737244 (2000 100%) 1.5346]\n",
      "Why,' and an an a heast strike great strecty \n",
      "came in chires of I \n",
      "can of they they \n",
      "any \n",
      "to fight -ul \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "  loss_ = train(*random_training_set())       \n",
    "  loss_avg += loss_\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "      all_losses.append(loss_avg / plot_every)\n",
    "      loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ee0so6aKJ5L8",
    "outputId": "6881fe64-d1ac-4300-f6df-eef607a60425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer String:   wh\n",
      " when a bright \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "had gate look and been a beave of \n",
      "then?' \n",
      "\n",
      "Burcues leat is a brat has the broken the Butter went turpped and \n",
      "call they pless of then hould \n",
      "could your have new, and there all the se \n",
      "\n",
      "Primer String:   Th\n",
      " There was not oud we hould ruapted and \n",
      "\n",
      "all \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bode \n",
      "to hazes. Bere what upon they \n",
      "at them as, and \n",
      "a'sure as \n",
      "Flowed and they be the Menay orehing did nearth of a least of them the stood or the gre \n",
      "\n",
      "Primer String:   lo\n",
      " long all streep the would glead, them \n",
      "hand. All beforge the passed, out low to not fairs death and the riders and saw there in the great fleft wround far all the Ends. But stood has them \n",
      "\n",
      "and bent pat \n",
      "\n",
      "Primer String:   wh\n",
      " where feen the Lord may have in as was the them fire and soon was spaine and the Butter up it and came out, and side for a leave the whore \n",
      "brown a the for, and the burnen down deed. The down all the ha \n",
      "\n",
      "Primer String:   I \n",
      " I heard. 'Yes, and they seemed as Araborn did now them be \n",
      "felt the ropain horsh and suppew them have \n",
      "still not slope off the looded with there the Ents -- the Lord road to felt \n",
      "as to shave pount ligh \n",
      "\n",
      "Primer String:   ca\n",
      " can and a way down about the seep the Enement. \n",
      "\n",
      "And need and they peat-peep without them all were sinding these is company been the erest a great been the Mcroward and the \n",
      "right. \n",
      "\n",
      "Pere a that you,' h \n",
      "\n",
      "Primer String:   Th\n",
      " Throught you the \n",
      "blights the bittle, and a bright the flowly. It said. At down them. \n",
      "\n",
      "When them and berel went some that the on and spright of \n",
      "Legain and not enemy fealt shire be?' \n",
      "\n",
      "'Hobbits. It gro \n",
      "\n",
      "Primer String:   lo\n",
      " look \n",
      "time the Lord Morrors and cource getter \n",
      "too behind silent distain. 'Yet the Light wither \n",
      "course eement, and pittle of the turned twight had ! Or have. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frodo bed than \n",
      "as the fice you not b \n",
      "\n",
      "Primer String:   I \n",
      " I treemed and a stone the \n",
      "trowed his close of Arvalf suven the tos, and them they can he swond they heard at them over the blaver \n",
      "the \n",
      "prealed them into the Don't have silt out to shadle and many that \n",
      "\n",
      "Primer String:   G\n",
      " Gandalf had stood and stoom not could be Sam boat stont of \n",
      "the greath, and seemed the the streppaning of Wild my as the Ento out perry pient the will and withou \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "peart \n",
      "here buircane, and I  \n",
      "\n",
      "Primer String:   wh\n",
      " where prese. But brow the darkle af could get \n",
      "them: \n",
      "\n",
      "I remord of the Men they hung whore one heard here Trught a was it fight will them speys have the bore that or has such the forth Sam \n",
      "wools he \n",
      "fo \n",
      "\n",
      "Primer String:   he\n",
      " he saw the Entains \n",
      "their Wimlone, and there went entween only flames the trove. \n",
      "\n",
      "Are dorn and \n",
      "pass in the glaps \n",
      "other and the beans of the great the smay betwee offer. \n",
      "\n",
      "'No, days, ever Undy, and th \n",
      "\n",
      "Primer String:   G\n",
      " Galden folk among his swairs and seem they purces the \n",
      "Now the \n",
      "respered and all they \n",
      "seemed go the great as the than the gats in the will soft a for the was saw up. Whan you \n",
      "all they should be the w \n",
      "\n",
      "Primer String:   ra\n",
      " rathered they some the door heard \n",
      "they hund shading little with had beding sind again the \n",
      "Shire, and the world now the Elves the did begul. We have they had the great sperst far into fear the Ento som \n",
      "\n",
      "Primer String:   G\n",
      " Gandalf. And know! A said \n",
      "spest in the Wickless have the mady upon the ring old usioung thear road, and \n",
      "some to powhered they seemed time. Do the till the Lodd that siince the Loght of the Dwards \n",
      "be \n",
      "\n",
      "Primer String:   ra\n",
      " rail. \n",
      "\n",
      "'I am they say. He wide rough that he tang \n",
      "own the laster, Mnatway. What the Black all \n",
      "of the garded Frodo shald, and said farter and patt the Horches of the from the them answered and \n",
      "seemed \n",
      "\n",
      "Primer String:   Th\n",
      " The long that the cloult for back \n",
      "came and speep that you faster have dark and untime they goods the \n",
      "know the feather for West wond the cloud, and one a his \n",
      "come and the Ele out \n",
      "from surpors of the  \n",
      "\n",
      "Primer String:   Th\n",
      " That fulled. In the drowing off othsing they rull we glast leared a they were were deep and melt Sable, the fut-tom their fell benow. In the blath. It days about of a greach \n",
      "for the rorg it and back of \n",
      "\n",
      "Primer String:   he\n",
      " he \n",
      "have they \n",
      "had and \n",
      "heas he seed all sights and the reathers. \n",
      "\n",
      "Are streckle they came the long for the was and them, and dady forgot as passily \n",
      "fall it. We leaked that a away many \n",
      "to \n",
      "curnen woul \n",
      "\n",
      "Primer String:   he\n",
      " he befould Beefor the fance, and they eopening taly the murth-see, from they was water, and seet mant at the mind first all before send the \n",
      "gone could been thought all \n",
      "of as Frodo, and were the Sou gr \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate some test examples\n",
    "for i in range(20):\n",
    "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
    "  start = random.randint(0,len(start_strings)-1)\n",
    "  print(\"Primer String: \", start_strings[start])\n",
    "  print(evaluate(start_strings[start], 200), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJhgDc2IauPE"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 6: Generate output on a different dataset\n",
    "\n",
    "---\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
    "\n",
    "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
    "\n",
    "**DONE:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "USMmd7NpOaqd",
    "outputId": "27de3f40-07f3-4c80-a74b-45d33d42c701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 466656\n"
     ]
    }
   ],
   "source": [
    "# Pick a new data set -> alma.txt  (From the Book of Mormon)\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "file = unidecode.unidecode(open('./text_files/alma.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWPNE63hOl5K"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "n_epochs = 2000\n",
    "print_every = 200\n",
    "plot_every = 10\n",
    "hidden_size = 200\n",
    "n_layers = 1\n",
    "lr = 0.001\n",
    " \n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "VWObddXORBMP",
    "outputId": "b9329f86-ea8c-4fa1-ff72-96922dd099a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.83067727088928 (200 10%) 2.1116]\n",
      "Wha Ye unto and me for ite sthut bedem the com me Lorut kur the lupled I so behid; berburth camang the \n",
      "\n",
      "[101.10603857040405 (400 20%) 1.4683]\n",
      "Whad in the to the preeding spomutess bight the jurder of the vent year unto unto this God of behold,  \n",
      "\n",
      "[151.471097946167 (600 30%) 1.4549]\n",
      "Whis many behold, our that shall the people.\n",
      "\n",
      " This sevile for the yelves time and the were soly house \n",
      "\n",
      "[201.62687373161316 (800 40%) 1.3712]\n",
      "Whriss of the forth younstand.\n",
      "\n",
      " And not should both and to people ih the caunsed the live that they h \n",
      "\n",
      "[251.9292299747467 (1000 50%) 1.0266]\n",
      "What ye to proched against and to sulfored common, and the ciest, and they sumber.\n",
      "\n",
      " Now we treit stal \n",
      "\n",
      "[302.2254202365875 (1200 60%) 1.3897]\n",
      "Whis freed they may in the should for you the Nephites in ye did breth to is them twender of God of Go \n",
      "\n",
      "[352.5074257850647 (1400 70%) 1.2809]\n",
      "Whi was are they people were overtancar the stroweth in the land me down sent that their in down and t \n",
      "\n",
      "[402.725754737854 (1600 80%) 1.2410]\n",
      "Whe our did many off my son, they armies if befone, they dess out much this sous, and he shall began t \n",
      "\n",
      "[452.9152970314026 (1800 90%) 1.2750]\n",
      "Where thems and church having should no mast, and the people of their land of God.\n",
      "\n",
      " And unto they sha \n",
      "\n",
      "[502.8979706764221 (2000 100%) 1.3277]\n",
      "When had been grant; and they beats of their more courses were is his freed and this words for out in  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "  loss_ = train(*random_training_set())       \n",
    "  loss_avg += loss_\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "      all_losses.append(loss_avg / plot_every)\n",
    "      loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "QrHObM0GRHat",
    "outputId": "f761805f-fbb8-4bde-c68e-73e5a7629fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:  And they did began to the enethren was all the wold of the word of God; for the people over the hands of\n",
      "Example 1:  And their did with the pright unto they did of the church eveath and had brethren, behold, the word of m\n",
      "Example 2:  And it be in the record a lardits, which was dist ye into the the seest behold, yea, and all the land no\n",
      "Example 3:  And he caused by the seed upon the church--Ammon, they were became out of mented the destroyed preserver\n",
      "Example 4:  And the words the would not sowards of the arms, and were men of their brethren your went of the Lamanit\n",
      "Example 5:  And upon and destroused which were all these many more receir of ye was our did not before the Lramanite\n",
      "Example 6:  And the Lord having command the our withed the church, and so destraties and dept king the Lord been des\n",
      "Example 7:  And the people of souls of God, and they were brethren with and rearth to the resire and the sincerness \n",
      "Example 8:  And the church, and also day; for the prepent of did brethren the delivered that he should be remence of\n",
      "Example 9:  And they have before they came to pass that them the land of Zarahemla, and preach the Lamanites behold,\n",
      "Example 10:  And the said unto the people to pass them were manner was his destroyed upti and destroy understrought p\n",
      "Example 11:  And the people and he had these a did not did all the secall the people might be came to pass that the b\n",
      "Example 12:  And their desire the good the land of God have into the sauld that the Lamanites were in the world beowe\n",
      "Example 13:  And the wildrened them, before the Lord outhore the could of all dies.\n",
      "\n",
      " And it came to pass that ye beg\n",
      "Example 14:  And behold, and saying: Whis with the words reit before the Lamanites with the land of Nephites and thei\n",
      "Example 15:  And it came to among the people of God to do that the land of the words of the Lord down it receivition \n",
      "Example 16:  And it came to pass that they distions.\n",
      "\n",
      " And the restrengthen their sins of the preaments of God miltio\n",
      "Example 17:  And he time.\n",
      "\n",
      " Nehil be been our halder or of the Lord the time that he could doth stare the priso; ther\n",
      "Example 18:  And they would be ae the word which hearts here did all them, and day.\n",
      "\n",
      " But behold, they did the people\n",
      "Example 19:  And it can all the land, they the people of the land whil berous things, were men his ends and he had ci\n"
     ]
    }
   ],
   "source": [
    "# Print test prime_str\n",
    "for i in range(20):\n",
    "  print(f\"Example {i}: \", evaluate('And ', predict_len=100, temperature = 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhuLNCXwmJrW"
   },
   "source": [
    "Evaluation of my Char-RNN Model:\n",
    "\n",
    "I decided to train my model on some of the text from the Book of Alma in the Book of Mormon. I think that my model did pretty well of learning words that are commonplace in the Book of Mormon. It was producing things like \"Nephites, Lamanites, Moroni, etc.\" which are unique to the book. This I think is cool and a good indicator that my model is learning structure and content/context of the training text.\n",
    "\n",
    "I think where my model struggles is in producing coherent sentences. It is able to copy sentence structure by producing periods and spaces (and sometimes even start and end quotes as seen when trained on LOTR). However, the sentences don't quite have any semantic meaning behind them. I think that is the biggest struggle of the model. But I am impressed that it was able to learn sentence structure and produce words that are would appear in the Book of Mormon, even if they aren't completely coherent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Lab6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
